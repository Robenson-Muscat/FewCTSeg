# FewCTSeg

## Context
CT-scans offer very precise 3D images of the human body (up to 0.5 mm resolution) and thus allow to capture the human anatomy.
The objective of this challenge is to automatically segment the anatomical structures of the human body, as well as tumors, on a CT-scan. In other words, it is about identifying the shapes visible on a CT-scan.
In the image below, from an abdominal CT scan, the different structures have been segmented:

![Example of an abdominal CT scan](images/raidium_2024_1.png).

## Goal

The goal of this challenge is to segment structures using their shape, but without exhaustive annotations.
The training data is composed of two types of images: partially annoted CT-scan images and raw CT-scan images

Partially annotated CT-scan images, with anatomical segmentation masks of individual structures, act as the field truth definition of what an anatomical structure is.
However, they are not supposed to be representative of all possible structures and their diversity, but can still be used as training material.
The masks do not contain all the organs annotated on the entire dataset. For example, on two abdominal images,
the mask for A will contain the liver and spleen, while the mask for B will only contain the spleen (while the liver is visible in the image).

Raw CT-scan images, without any segmented structure can be used as additional training material, in the context of unsupervised training.

The test set is composed of new images with all the corresponding segmented structures, and the metric measures the ability to correctly segment and separate the different structures on an image.


---

## UniMatch : a unique semi-supervised semantic segmentation technique

[UniMatch](https://arxiv.org/pdf/2208.09910) is a efficient novel deep learning framework that can be used to train semantic segmentation models in medical imaging when labels are limited and uses unlabeled images as extra training data under a consistencyâ€‘regularization framework (assumption that prediction of an unlabeled example should be invariant to different forms of perturbations). This method combines three consistency streams:

1. **Weak stream**  
   - Weak perturbations : geometric perturbation(crop, rotation) â†’ generate pseudo-labels from the model trained on labeled data .

2. **Featureâ€‘perturbed stream**  
   - Dropout on encoder features â†’ featureâ€‘consistency loss.

3. **Strong streams**  
   - Two strong perturbations from a non-determinstic augmentation(Cutout) â†’ imageâ€‘consistency loss  
   - Pseudoâ€‘labels from the weak stream guide strongâ€‘view predictions.
  
These three streams should probalistically as close as possible to output a similar mask. Predictions are trained to match weak-stream pseudo-labels.

![UniMatch](images/unimatch_frame.png).


### ğŸ”§ Our Adaptation

1. **Backbone & head**  
   - SegFormer architecture (pretrained)

2. **Data splits**  
   - 80â€¯% labeled â†’ training 
   - 20â€¯% labeled â†’ validation  
   - All emptyâ€‘mask images â†’ unlabeled pool

3. **Strong augmentations**  
   - **Cutout**
    
4. **Pseudoâ€‘label filtering of unlabeled images**  
   - Pixelâ€‘wise confidence â‰¥â€¯Ï„ â†’ assign class; others receive a sentinel IGNORE_INDEX value and are not used for unsupervised CE.
   - We optionally report `fraction_kept` per batch/epoch for monitoring.
  
5. **First phase : Supervised training**
   - Training with a $L_{sup} =Dice(y,Å·)$ with $y$ groundâ€‘truth mask and $Å·$  predicted logits of a labeled image $x$
   - 
6. **Seconde phase : semiâ€‘supervised training**  
   - Combine labeled + pseudoâ€‘labeled sets in a mixed batch  
   - LR scheduling via `ReduceLROnPlateau`
   - Training with a $L =0.5 ( L_{sup} + L_{unsup})$


### ğŸ“ Unsupervised Loss Details

Let  

$x_u$  an input unlabeled image, $Å·_w$ weakâ€‘stream logits, $Å·_{fp}$ featureâ€‘perturbed logits, $Å·_{s1}$, $Å·_{s2}$ strongâ€‘stream logits and $áº$ pseudoâ€‘label from weak stream (Pixelâ€‘wise confidence â‰¥â€¯Ï„ â†’ assign class).


The unsupervised loss is
$L_{unsup}  =  Î»Â·L_{fp} + Î¼Â·L_s$

where

- $L_{fp}$:  Cross-entropy loss between feature-perturbed logits and weak pseudoâ€‘labels ($L_{fp}  = CE(Å·_{fp}, áº)$)

- $L_s$ : Average Cross-entropy between each strongâ€‘view and weak pseudoâ€‘labels ($L_{img} =0.5 [CE(Å·_{s1}, áº) + CE(Å·_{s2}, áº)]$)

- Î», Î¼: Weighting hyperparameters

