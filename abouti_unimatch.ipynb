{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eSZEcNo1byf",
        "outputId": "7a15bb2d-61a5-40b2-841f-124a82057279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.6.15)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wEeMrJofCCNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a15d6b8-23b6-41b9-be45-7cad46c1de8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "METHODE A LA CHATGPT"
      ],
      "metadata": {
        "id": "XG1KFdkVCH77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, glob, json\n",
        "import numpy as np, pandas as pd, cv2\n",
        "import torch, random\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "SEED = 26\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PATH = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "IMG_DIR = os.path.join(PATH, 'train-images/')\n",
        "MASK_CSV = os.path.join(PATH, 'y_train.csv')\n",
        "MODEL_OUT = PATH\n",
        "BATCH = 8\n",
        "NUM_CLASSES = 55\n",
        "NUM_EPOCHS_PHASE1 = 10\n",
        "NUM_EPOCHS_PHASE2 = 25\n",
        "PSEUDO_LABEL_THRESHOLD = 0.6\n",
        "LAMBDA = 0.5  # weight feature-perturb loss\n",
        "MU = 0.5      # weight image-level loss\n",
        "IGNORE_INDEX = 255  # for unlabeled pixels\n",
        "\n",
        "# Fix randomness\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "def alphanumeric_sort(name):\n",
        "    parts = re.split(r'(\\d+)', name)\n",
        "    return [int(p) if p.isdigit() else p for p in parts]\n",
        "\n",
        "# ------------------ Custom CutMix ------------------\n",
        "def cutmix(images, masks):\n",
        "    # images, masks: tensors shape (B,C,H,W),(B,H,W)\n",
        "    B, C, H, W = images.shape\n",
        "    lam = np.random.beta(1.0, 1.0)\n",
        "    rx = np.random.randint(W)\n",
        "    ry = np.random.randint(H)\n",
        "    rw = int(W * np.sqrt(1 - lam))\n",
        "    rh = int(H * np.sqrt(1 - lam))\n",
        "    x1 = np.clip(rx - rw // 2, 0, W)\n",
        "    y1 = np.clip(ry - rh // 2, 0, H)\n",
        "    x2 = np.clip(rx + rw // 2, 0, W)\n",
        "    y2 = np.clip(ry + rh // 2, 0, H)\n",
        "    perm = torch.randperm(B)\n",
        "    mixed_images = images.clone()\n",
        "    mixed_masks = masks.clone()\n",
        "    mixed_images[:, :, y1:y2, x1:x2] = images[perm, :, y1:y2, x1:x2]\n",
        "    mixed_masks[:, y1:y2, x1:x2] = masks[perm, y1:y2, x1:x2]\n",
        "    return mixed_images, mixed_masks\n",
        "\n"
      ],
      "metadata": {
        "id": "LuliALC0snKx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Transforms ------------------\n",
        "base_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2()])\n",
        "\n",
        "weak_transform = A.Compose([\n",
        "    A.RandomResizedCrop((256,256), scale=(0.2,1.0), p=1.0),\n",
        "    #A.RandomRotate90(p=0.5),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2()])\n",
        "\n"
      ],
      "metadata": {
        "id": "F9oHPiQ0GdOv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strong_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2()])  # CutMix applied in batch"
      ],
      "metadata": {
        "id": "GuHIErrKHCcQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Datasets ------------------\n",
        "class LabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, transform):\n",
        "        paths = sorted(glob.glob(os.path.join(img_dir, '*.png')), key=alphanumeric_sort)\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid = [m.sum()>0 for m in masks]\n",
        "        self.image_paths = [p for p,v in zip(paths,valid) if v]\n",
        "        self.masks = masks[np.array(valid)]\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        mask = self.masks[idx]\n",
        "        aug = self.transform(image=img, mask=mask)\n",
        "        return aug['image'], aug['mask']\n",
        "\n",
        "class UnlabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, weak_transform):\n",
        "        paths = sorted(glob.glob(os.path.join(img_dir, '*.png')), key=alphanumeric_sort)\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid = [m.sum()==0 for m in masks]\n",
        "        self.image_paths = [p for p,v in zip(paths,valid) if v]\n",
        "        self.transform = weak_transform\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        aug = self.transform(image=img)\n",
        "        return aug['image'], self.image_paths[idx]\n",
        "\n",
        "class PseudoCTScanDataset(Dataset):\n",
        "    def __init__(self, image_paths, masks):\n",
        "        self.image_paths, self.masks = image_paths, masks\n",
        "    def __len__(self): return len(self.masks)\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        aug = base_transform(image=img, mask=self.masks[idx])\n",
        "        #aug = strong_transform(image=img, mask=self.masks[idx])\n",
        "        return aug['image'], aug['mask']\n"
      ],
      "metadata": {
        "id": "bgRV0ybXHkNy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ DataLoaders ------------------\n",
        "# Phase1 : Split Train/val (reprodcuible)\n",
        "\n",
        "full_lab = LabeledCTScanDataset(IMG_DIR, MASK_CSV, base_transform)\n",
        "idxs = torch.randperm(len(full_lab), generator=torch.Generator().manual_seed(SEED)).tolist()\n",
        "#idxs = np.random.default_rng(seed=SEED).permutation(len(full_lab))\n",
        "split = int(0.8*len(full_lab))\n",
        "train_ds = Subset(full_lab, idxs[:split])\n",
        "val_ds   = Subset(full_lab, idxs[split:])\n",
        "gen = torch.Generator(); gen.manual_seed(SEED)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4, generator=gen)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "unlab_ds = UnlabeledCTScanDataset(IMG_DIR, MASK_CSV, weak_transform)\n",
        "unlab_loader = DataLoader(unlab_ds, batch_size=BATCH, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hntdqbhSGkl-",
        "outputId": "35f6c62a-cbd2-469a-d622-ffc74c0cd4b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TM6UdTcs89xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.Segformer(\n",
        "    encoder_name='timm-efficientnet-b7', encoder_weights='imagenet',\n",
        "    in_channels=3, classes=NUM_CLASSES\n",
        ").to(DEVICE)\n",
        "sup_loss = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "opt1 = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "5e6ff2b3bf234508b355dfd7ef35a2f0",
            "844a881786ba481786ea053e3841020c",
            "31cf7cf88d944391be747129652c924e",
            "89e30304a9714bd7bc7ef232ec3aa42a",
            "4c48e89397574068bc9c546eb43682dd",
            "617c79783719474db5fdc3b0558d321f",
            "869ba79974094bd18fe207b284140df1",
            "c3ed70e6ea4f4dac8b2f0bdb0b74aaa9",
            "78c12c8de646424982cb3edfd0288261",
            "5b13e72359f94d298d0e9147c9843abc",
            "247cefd1cca4447cac05f4d661981bfa",
            "0df51014a3cb4409b5d31de150eedd78",
            "83803dcec02e4fb8ba0599289ab9cd16",
            "34532f89291745bb972f302e4be3b392",
            "502b95d695ee49379703df1130e625dc",
            "44012c35eb45403ab35e0f4b3e164609",
            "172800da2eeb40b08f1d5b05ce31a2e6",
            "4712bb2141464f7aa3aff0baf3c2b139",
            "10fcf0b0a9b5457ab7f5a9f71f5d5d65",
            "f04242f63865415f84d14dd85ce725d3",
            "472c10561cb048859821ffc7fffec819",
            "a79e14d5391c4001b5ba9c5bd484ce65"
          ]
        },
        "id": "nyBJ9FtRuXHu",
        "outputId": "3d2f88e1-275e-4b7f-efa8-0070bc515623"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/106 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e6ff2b3bf234508b355dfd7ef35a2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/267M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0df51014a3cb4409b5d31de150eedd78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SLKhpxo5gsp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Phase1: Warmup ------------------\n",
        "best_val=1e6\n",
        "for epoch in range(NUM_EPOCHS_PHASE1):\n",
        "    model.train(); train_loss=0\n",
        "    for imgs, masks in tqdm(train_loader):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        opt1.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = sup_loss(logits, masks.long())\n",
        "        loss.backward(); opt1.step()\n",
        "        train_loss += loss.item()*imgs.size(0)\n",
        "    val_loss=0; model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs,masks=imgs.to(DEVICE),masks.to(DEVICE)\n",
        "            val_loss+=sup_loss(model(imgs), masks.long()).item()*imgs.size(0)\n",
        "    val_loss /= len(val_ds)\n",
        "    print(f\"Epoch1 {epoch+1}/{NUM_EPOCHS_PHASE1} — val: {val_loss:.4f}\")\n",
        "    if val_loss<best_val:\n",
        "        best_val=val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_OUT,'best_phase1_unimatch_3006.pth'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5PnmB0lsosJ",
        "outputId": "7c73fb1a-2141-437a-d1bd-42bde5f01c01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:40<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 1/10 — val: 0.3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 2/10 — val: 0.3031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 3/10 — val: 0.2905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:14<00:00,  5.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 4/10 — val: 0.2850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 5/10 — val: 0.2683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 6/10 — val: 0.2687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 7/10 — val: 0.2716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:12<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 8/10 — val: 0.2732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:11<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 9/10 — val: 0.2651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:15<00:00,  4.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1 10/10 — val: 0.2670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_state_dict(torch.load(os.path.join(MODEL_OUT,'best_phase1_unimatch_3006.pth')))\n",
        "model.eval()\n",
        "pseudo_paths, pseudo_masks = [], []"
      ],
      "metadata": {
        "id": "Cj7N2rb5tWIM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Pseudo-labeling ------------------\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in tqdm(unlab_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        pw = torch.softmax(model(imgs), dim=1)\n",
        "        conf, pred = pw.max(1)\n",
        "        for i,pth in enumerate(paths):\n",
        "            conf_i, pred_i = conf[i].cpu().numpy(), pred[i].cpu().numpy().astype(np.uint8)\n",
        "            mask_i = np.where(conf_i>=PSEUDO_LABEL_THRESHOLD, pred_i, 0)\n",
        "            if (conf_i>=PSEUDO_LABEL_THRESHOLD).mean()>=LAMBDA:\n",
        "                pseudo_paths.append(pth); pseudo_masks.append(mask_i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgXRdmAjtg7F",
        "outputId": "37248647-7fe2-4506-cc41-683ebdc4ffb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 156/156 [00:08<00:00, 17.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si vide --> skip phase 2\n",
        "if len(pseudo_masks)==0:\n",
        "    raise ValueError(\"No pseudo-labels found, adjust thresholds.\")\n",
        "pseudo_masks = np.stack(pseudo_masks,0)\n",
        "pseudo_ds = PseudoCTScanDataset(pseudo_paths, pseudo_masks)\n"
      ],
      "metadata": {
        "id": "7lCWGIzW7Gx9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pseudo_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C47fm6wDwjaX",
        "outputId": "93b59c15-adad-4b24-c204-1d2ed6d59d4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1241"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Phase2: UniMatch ------------------\n",
        "joint_ds = ConcatDataset([train_ds, pseudo_ds])\n",
        "joint_loader = DataLoader(joint_ds, batch_size=BATCH, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "_Wg6pQb47Az8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Hyperparameters of UniMatch ------------------\n",
        "opt2 = Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(opt2, 'min', factor=0.5, patience=3)\n",
        "drop2d = nn.Dropout2d(p=0.5)\n",
        "dice = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "ce = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "std, mean = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "N9TVX0G3tlSQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PSEUDO_LABEL_THRESHOLD = 0.6"
      ],
      "metadata": {
        "id": "O2HWpURMabln"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Création loader de validation pour phase2 identique à val_loader\n",
        "for epoch in range(1, NUM_EPOCHS_PHASE2+1):\n",
        "\n",
        "    model.train(); train_loss=0\n",
        "    for imgs, masks in tqdm(joint_loader):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        # CutMix sur batch supervisé\n",
        "        imgs, masks = cutmix(imgs, masks)\n",
        "        # Forward weak\n",
        "        feat_w = model.encoder(imgs)\n",
        "        logits_w = model.segmentation_head(model.decoder(feat_w))\n",
        "        # Feature perturbation\n",
        "        last = feat_w[-1]; feat_fp = list(feat_w)\n",
        "        feat_fp[-1] = drop2d(last)\n",
        "        logits_fp = model.segmentation_head(model.decoder(feat_fp))\n",
        "\n",
        "        # Two strong streams\n",
        "        # (we reuse mix outputs as strong; for demo)\n",
        "        logits_s1 = model(imgs); logits_s2 = model(imgs)\n",
        "        # Pseudo mask from weak augmentation (detach)\n",
        "        pseudo = logits_w.argmax(1).detach()\n",
        "\n",
        "        # 2) Flux fort\n",
        "        \"\"\"\n",
        "        Mettre dans une fonction\n",
        "        # a) récupérer numpy non-normalisé\n",
        "        imgs_np = imgs.cpu().permute(0,2,3,1).numpy()\n",
        "        imgs_np = (imgs_np * std + mean) * 255.0  # repasser en 0–255\n",
        "\n",
        "        # b) deux augmentations indépendantes\n",
        "        xs1 = torch.stack([strong_transform(image=img.astype(np.uint8))['image']\n",
        "                            for img in imgs_np]).to(DEVICE)\n",
        "        xs2 = torch.stack([strong_transform(image=img.astype(np.uint8))['image']\n",
        "                            for img in imgs_np]).to(DEVICE)\n",
        "        \"\"\"\n",
        "\n",
        "        # Losses\n",
        "        loss_sup = sup_loss(logits_w, masks.long())\n",
        "\n",
        "        loss_fp = ce(logits_fp, pseudo)\n",
        "        loss_s = 0.5*(ce(logits_s1, pseudo) + ce(logits_s2, pseudo))\n",
        "\n",
        "        loss_u = LAMBDA*loss_fp + MU*loss_s\n",
        "        loss = 0.5*(loss_sup + loss_u)\n",
        "        opt2.zero_grad(); loss.backward(); opt2.step()\n",
        "        train_loss += loss.item()*imgs.size(0)\n",
        "    train_loss /= len(joint_ds)\n",
        "\n",
        "    # Validation phase2\n",
        "    model.eval(); val_loss=0\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "            val_loss += sup_loss(model(imgs), masks.long()).item()*imgs.size(0)\n",
        "    val_loss /= len(val_ds)\n",
        "    scheduler.step(val_loss)\n",
        "    print(f\"Phase2 E{epoch}/{NUM_EPOCHS_PHASE2} — train:{train_loss:.4f} val:{val_loss:.4f}\")\n",
        "    if epoch%5==0:\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_OUT, f'phase2_e{epoch}_unimatch_drp05_th0.9_3006.pth'))\n",
        "    if val_loss<best_val:\n",
        "        best_val=val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_OUT,'best_phase2_unimatch_drp05_th0.9_3006.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "hQrSa4OFtM2R",
        "outputId": "e1febcb0-94f5-4b07-bab0-dced2632b447"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:32<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E1/25 — train:0.2645 val:0.2534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:31<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E2/25 — train:0.2568 val:0.2597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:32<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E3/25 — train:0.2563 val:0.2542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:31<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E4/25 — train:0.2555 val:0.2565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:32<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E5/25 — train:0.2529 val:0.2612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:31<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E6/25 — train:0.2498 val:0.2628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [01:32<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 E7/25 — train:0.2497 val:0.2627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 8/231 [00:03<01:48,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34-1609382333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLAMBDA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_fp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMU\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_sup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_u\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Rajouter le 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mopt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(MODEL_OUT,'best_phase2_final_unimatch_3006.pth'))"
      ],
      "metadata": {
        "id": "HVp6LzRJVhlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dm03ts1Eso04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 27/06 - CHATGPT"
      ],
      "metadata": {
        "id": "uIgvjGoqKeAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, glob, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import albumentations as A\n",
        "import segmentation_models_pytorch as smp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "SEED = 26\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Paths\n",
        "PATH = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "IMG_DIR = os.path.join(PATH, 'train-images/')\n",
        "MASK_CSV = os.path.join(PATH, 'y_train.csv')\n",
        "MODEL_OUT = PATH\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH = 8\n",
        "NUM_CLASSES = 55\n",
        "NUM_EPOCHS_PHASE1 = 20\n",
        "NUM_EPOCHS_PHASE2 = 20\n",
        "#PSEUDO_LABEL_THRESHOLD = 0.6\n",
        "PSEUDO_LABEL_THRESHOLD = 0.9\n",
        "LAMBDA = 0.5    # weight for feature perturbation loss\n",
        "MU = 0.5        # weight for image-level loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AqS8X1uXCDnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Utilities ------------------\n",
        "def alphanumeric_sort(name):\n",
        "    parts = re.split(r'(\\d+)', name)\n",
        "    return [int(p) if p.isdigit() else p for p in parts]\n",
        "\n",
        "# ------------------ Datasets ------------------\n",
        "class LabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, transform):\n",
        "        self.paths = sorted(glob.glob(os.path.join(img_dir, '*.png')), key=alphanumeric_sort)\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid = [m.sum()>0 for m in masks]\n",
        "        self.image_paths = [p for p,v in zip(self.paths, valid) if v]\n",
        "        self.masks = masks[np.array(valid)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        mask = self.masks[idx]\n",
        "        aug = self.transform(image=img, mask=mask)\n",
        "        return aug['image'], aug['mask']\n",
        "\n",
        "class UnlabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, weak_transform):\n",
        "        self.paths = sorted(glob.glob(os.path.join(img_dir, '*.png')), key=alphanumeric_sort)\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid = [m.sum()==0 for m in masks]\n",
        "        self.image_paths = [p for p,v in zip(self.paths, valid) if v]\n",
        "        self.transform = weak_transform\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        aug = self.transform(image=img)\n",
        "        return aug['image'], os.path.basename(path)\n",
        "\n",
        "class PseudoCTScanDataset(Dataset):\n",
        "    def __init__(self, image_paths, masks, transform):\n",
        "        self.image_paths = image_paths\n",
        "        self.masks = masks\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.masks)\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        aug = self.transform(image=img, mask=self.masks[idx])\n",
        "        return aug['image'], aug['mask']\n"
      ],
      "metadata": {
        "id": "ol8ubUeXRzFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Transforms ------------------\n",
        "base_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2()])\n",
        "\n",
        "weak_transform = A.Compose([\n",
        "    A.RandomResizedCrop((256,256),scale = (0.2,1), p=1.0),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    #A.OneOf([\n",
        "        #A.ElasticTransform(alpha=300, sigma=10, p=0.5),\n",
        "        #A.GridDistortion(distort_limit=0.2, num_steps=5, p=0.5)],p=1.0),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2()])\n",
        "\n",
        "strong_transform = A.Compose([\n",
        "    #A.RandomResizedCrop((256,256),scale = (0.2,1), p=1.0),\n",
        "    A.CoarseDropout(num_holes_range=(4, 8), hole_height_range=(0.1, 0.25),hole_width_range=(0.1, 0.25), fill_value=0, p=1.0),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2()])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "40o64f68HRXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b52e6c-243c-42de-c8c9-8a069268e99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2777448947.py:18: UserWarning: Argument(s) 'fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(num_holes_range=(4, 8), hole_height_range=(0.1, 0.25),hole_width_range=(0.1, 0.25), fill_value=0, p=1.0),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ DataLoaders ------------------\n",
        "# Phase 1: supervised\n",
        "full_lab = LabeledCTScanDataset(IMG_DIR, MASK_CSV, base_transform)\n",
        "idxs = np.random.permutation(len(full_lab))\n",
        "#idxs = np.random.default_rng(seed=SEED).permutation(len(full_lab))\n",
        "split = int(0.8*len(full_lab))\n",
        "train_ds = Subset(full_lab, idxs[:split])\n",
        "val_ds   = Subset(full_lab, idxs[split:])\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "# Unlabeled loader for pseudo-label generation\n",
        "unlab_ds = UnlabeledCTScanDataset(IMG_DIR, MASK_CSV, weak_transform)\n",
        "unlab_loader = DataLoader(unlab_ds, batch_size=BATCH, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "Qox8sS64Ch48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs"
      ],
      "metadata": {
        "id": "K8dj2PD_8UCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Model & Loss ------------------\n",
        "model = smp.Segformer(\n",
        "    encoder_name='timm-efficientnet-b7', encoder_weights='imagenet',\n",
        "    in_channels=3, classes=NUM_CLASSES, activation=None\n",
        ").to(DEVICE)\n",
        "\n",
        "loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "93pg7a6TCejW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "2079a0f9a6774930a4cd354ad62a1851",
            "c187bd4f62044a09ab03342b6fbbaf1a",
            "5a69e5f66442413287d917043e5c443a",
            "297687afb346474eb83ba6d27ffa6f61",
            "b2df93a9c8ab4ab69da785a851166bf4",
            "d178fe1ea36545b98820d502daf2a2e2",
            "e18c366f2d334fc8b184f5172939647a",
            "9838ad30cc564bbeb2274d37e3a358b8",
            "0a305423b90a4623ac0fb045a9db61bb",
            "b827ab012e8d42718b66a90f8daca307",
            "32016ab5b3f049f6b8baa322982a8a98",
            "d49c919d3aa0407888ffb8c7ecc5aa8d",
            "d5f4bba395f545c1ada03c1f7d9bf1f6",
            "f9f1e26539a0445dad72f96e8c7b3cea",
            "6c5bd7237d474c7cb75e0d0ae9776032",
            "f4a66aaa174a47b4bea5db16db0f9623",
            "2d4de8a58043465399ec54a007f74195",
            "13f0a56fc5fa488ea0bf26e0421a8e92",
            "cd31b9d2f73e40a3af6c38e74d57fcc9",
            "24ab1aa6916c4f3b963048ffd0ea6b53",
            "070c5efd3a22416593abddd73c5dbcca",
            "1f54a26692124a6f8eca390e35abb26f"
          ]
        },
        "outputId": "76b35b48-5c5f-4495-90ed-2530e52df4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/106 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2079a0f9a6774930a4cd354ad62a1851"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/267M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d49c919d3aa0407888ffb8c7ecc5aa8d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Phase 1: Warm-up ----------------\n",
        "best_val = float('inf')\n",
        "for epoch in range(NUM_EPOCHS_PHASE1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Phase1 Epoch {epoch+1}\"):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = loss_fn(logits, masks.long())\n",
        "        loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()*imgs.size(0)\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "            val_loss += loss_fn(model(imgs), masks.long()).item()*imgs.size(0)\n",
        "    val_loss /= len(val_ds)\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_OUT,'best_phase1_2706.pth'))\n",
        "\n"
      ],
      "metadata": {
        "id": "DDk1nO3jCbY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86888ed3-4b0c-47fd-c5ed-4651122ae352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 1: 100%|██████████| 76/76 [00:34<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 2: 100%|██████████| 76/76 [00:12<00:00,  6.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 3: 100%|██████████| 76/76 [00:12<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 4: 100%|██████████| 76/76 [00:15<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 5: 100%|██████████| 76/76 [00:11<00:00,  6.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 6: 100%|██████████| 76/76 [00:15<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 7: 100%|██████████| 76/76 [00:12<00:00,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 8: 100%|██████████| 76/76 [00:15<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 9: 100%|██████████| 76/76 [00:15<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 10: 100%|██████████| 76/76 [00:12<00:00,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 11: 100%|██████████| 76/76 [00:11<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 12: 100%|██████████| 76/76 [00:11<00:00,  6.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 13: 100%|██████████| 76/76 [00:11<00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 14: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 15: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 16: 100%|██████████| 76/76 [00:11<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 17: 100%|██████████| 76/76 [00:12<00:00,  6.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 18: 100%|██████████| 76/76 [00:11<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 19: 100%|██████████| 76/76 [00:11<00:00,  6.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase1 Epoch 20: 100%|██████████| 76/76 [00:11<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Generate Pseudo-Labels ------------------\n",
        "model.load_state_dict(torch.load(os.path.join(MODEL_OUT,'best_phase1_2706.pth')))\n",
        "model.eval()\n",
        "pass"
      ],
      "metadata": {
        "id": "sgoP_9ULCXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_paths, pseudo_masks = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(unlab_loader, desc=\"Pseudo-labeling\"):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        # Dual strong views\n",
        "        xs1 = torch.stack([strong_transform(image=img.cpu().permute(1,2,0).numpy())['image'] for img in imgs]).to(DEVICE)\n",
        "        xs2 = torch.stack([strong_transform(image=img.cpu().permute(1,2,0).numpy())['image'] for img in imgs]).to(DEVICE)\n",
        "        # Weak features\n",
        "        pw_logits = model(imgs)\n",
        "        pw = torch.softmax(pw_logits, dim=1)\n",
        "        conf, mask_pred = pw.max(dim=1)\n",
        "        for i,name in enumerate(names):\n",
        "            conf_i = conf[i].cpu().numpy()\n",
        "            pred_i = mask_pred[i].cpu().numpy().astype(np.uint8)\n",
        "            mask_i = np.where(conf_i>=PSEUDO_LABEL_THRESHOLD, pred_i, 0)\n",
        "            keep_ratio = (conf_i>=PSEUDO_LABEL_THRESHOLD).mean()\n",
        "\n",
        "            if keep_ratio >= LAMBDA:\n",
        "                pseudo_paths.append(unlab_ds.image_paths[i])\n",
        "                pseudo_masks.append(mask_i)\n",
        "\n",
        "            #pseudo_paths.append(unlab_ds.image_paths[i])\n",
        "            #pseudo_paths.append(os.path.join(IMG_DIR,name))\n",
        "            #pseudo_masks.append(mask_i)\n"
      ],
      "metadata": {
        "id": "CD9OJSpnCUQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739df7ef-4c4d-4c6e-eef3-7b4b9efcf39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pseudo-labeling: 100%|██████████| 156/156 [00:16<00:00,  9.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_masks = np.stack(pseudo_masks,axis=0)"
      ],
      "metadata": {
        "id": "u6jDf9SDY8Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_masks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3sWBrHbh8y",
        "outputId": "6c802b13-b4a9-420b-a255-f8b9c59bd6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1241, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pseudo_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Di1qcPULPg",
        "outputId": "cfdfc4fa-cc5f-4d94-acf5-67b069626143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1241"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pseudo dataset\n",
        "pseudo_ds = PseudoCTScanDataset(pseudo_paths, pseudo_masks, strong_transform)\n"
      ],
      "metadata": {
        "id": "ltmYVVnzCR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Phase 2: UniMatch ------------------\n",
        "# Combine datasets\n",
        "joint_ds = ConcatDataset([train_ds, pseudo_ds])\n",
        "joint_loader = DataLoader(joint_ds, batch_size=BATCH, shuffle=True, num_workers=4)\n",
        "optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "#scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n"
      ],
      "metadata": {
        "id": "mJZ7jWBBIwLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout2d =nn.Dropout2d(p=0.3)\n",
        "dice_loss = smp.losses.DiceLoss(mode='multiclass', from_logits=True)"
      ],
      "metadata": {
        "id": "zjt-usAYiQq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1,11):\n",
        "    model.train(); tot_loss=0\n",
        "    for imgs, masks in tqdm(joint_loader, desc=f\"Phase2 Epoch {epoch}\"):\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        # Weak stream\n",
        "        feat_w = model.encoder(imgs)\n",
        "        #p_w = model.decoder(feat_w)\n",
        "        p_w  = model.segmentation_head(model.decoder(feat_w))\n",
        "\n",
        "        # Feature perturb stream\n",
        "        #feat_fp = nn.Dropout2d(p=0.5)(feat_w)\n",
        "        \"\"\"\n",
        "\n",
        "        last_feat = feat_w[-1]            # par ex. la feature la plus “profond”\n",
        "        perturbed = drop(last_feat)\n",
        "        p_fp = model.decoder([*feat_w[:-1], perturbed])\n",
        "        \"\"\"\n",
        "        last_w  = feat_w[-1]                # map finale (B, C, h, w)\n",
        "        last_fp = dropout2d(last_w)          # feature perturbée\n",
        "        feat_fp = feat_w.copy()\n",
        "        feat_fp[-1] = last_fp\n",
        "        #p_fp = model.decoder(feat_fp)\n",
        "        p_fp  = model.segmentation_head(model.decoder(feat_fp))\n",
        "\n",
        "        # Dual strong image streams\n",
        "        xs1 = torch.stack([strong_transform(image=img.cpu().permute(1,2,0).numpy())['image'] for img in imgs]).to(DEVICE)\n",
        "        xs2 = torch.stack([strong_transform(image=img.cpu().permute(1,2,0).numpy())['image'] for img in imgs]).to(DEVICE)\n",
        "\n",
        "        p_s1 = model(xs1); p_s2 = model(xs2)\n",
        "        # Pseudo mask from p_w\n",
        "        pseudo_mask = p_w.argmax(dim=1).detach()\n",
        "        # Losses\n",
        "\n",
        "        #ce = nn.CrossEntropyLoss()\n",
        "        #Après tester avec la cross entropy\n",
        "\n",
        "        loss_fp = dice_loss(p_fp, pseudo_mask.long())\n",
        "        loss_s = (dice_loss(p_s1, pseudo_mask.long()) + dice_loss(p_s2, pseudo_mask.long())) * 0.5\n",
        "        loss_u = LAMBDA * loss_fp + MU * loss_s\n",
        "        loss = loss_u\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot_loss += loss.item()*imgs.size(0)\n",
        "    avg_loss = tot_loss/len(joint_ds)\n",
        "\n",
        "    print(f\"Phase2 Train Loss: {avg_loss:.4f}\")\n",
        "    if epoch %5 ==0:\n",
        "\n",
        "      torch.save(model.state_dict(), os.path.join(MODEL_OUT, f'best_phase2_epoch_{epoch}_2706.pth'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CmixBc6tCDrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c279f6b-5e28-43a4-ac20-ea9314485ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 1: 100%|██████████| 231/231 [01:43<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.2275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 2: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 3: 100%|██████████| 231/231 [01:43<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 4: 100%|██████████| 231/231 [01:43<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 5: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 6: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 7: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 8: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 9: 100%|██████████| 231/231 [01:42<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Phase2 Epoch 10: 100%|██████████| 231/231 [01:42<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase2 Train Loss: 0.1149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjUWuw_anjBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reesayer comme ça"
      ],
      "metadata": {
        "id": "xoyAwpF-CDuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js0ngHpECDw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pFq6L7DCDzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gIXVUGEoCD1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uge834F4CD3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr-l5iN5CD54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4y_UcGntCD8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1BSwPJsCD_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PoJCp6lGnee"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import cv2\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split, Subset, Dataset, ConcatDataset\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json, os, glob, re\n",
        "import numpy as np, pandas as pd\n",
        "import albumentations as A\n",
        "import segmentation_models_pytorch as smp\n",
        "#cv2.setNumThreads(0)  - To avoid slower computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBCs6e12oG-T"
      },
      "outputs": [],
      "source": [
        "SEED = 26\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PATH = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "IMG_DIR = \"/content/drive/MyDrive/FewCTSeg/data/train-images/\"\n",
        "MASK_CSV = \"/content/drive/MyDrive/FewCTSeg/data/y_train.csv\"\n",
        "BATCH = 8\n",
        "NUM_CLASSES = 55\n",
        "NUM_EPOCHS = 1\n",
        "PSEUDO_LABEL_THRESHOLD = 0.9\n",
        "\n",
        "LAMBDA = 0.5\n",
        "MU = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxxZYdNusaoE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uInz2BoKry8b"
      },
      "outputs": [],
      "source": [
        "def alphanumeric_sort(name):\n",
        "    parts = re.split(r'(\\d+)', name)\n",
        "    return [int(p) if p.isdigit() else p for p in parts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dSzPqoDrtFv"
      },
      "outputs": [],
      "source": [
        "class LabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, transform):\n",
        "        all_paths = sorted(glob.glob(os.path.join(img_dir, \"*.png\")), key=alphanumeric_sort)\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid_idx = np.array([m.sum() > 0 for m in masks])\n",
        "        # on filtre image_paths et masks\n",
        "        filtered_paths = [p for p, valid in zip(all_paths, valid_idx) if valid]\n",
        "        filtered_masks = masks[valid_idx]\n",
        "\n",
        "        self.image_paths = filtered_paths\n",
        "        self.masks = filtered_masks\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    def __getitem__(self, i):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        mask = self.masks[i]\n",
        "        aug = self.transform(image=img, mask=mask)\n",
        "        return aug['image'], aug['mask']\n",
        "\n",
        "        return img_t, torch.zeros((256,256), dtype=torch.long)\n",
        "\n",
        "class UnlabeledCTScanDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_csv, transform):\n",
        "        all_paths = sorted(glob.glob(os.path.join(img_dir,\"*.png\")), key=alphanumeric_sort)\n",
        "\n",
        "\n",
        "        masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1,256,256).astype(np.uint8)\n",
        "        valid_idx = np.array([m.sum() == 0 for m in masks])\n",
        "        # on filtre image_paths et masks\n",
        "        filtered_paths = [p for p, valid in zip(all_paths, valid_idx) if valid]\n",
        "        filtered_masks = masks[valid_idx]\n",
        "\n",
        "        self.image_paths = filtered_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.image_paths[i]\n",
        "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        img_t = self.transform(image=img)['image']\n",
        "        name = os.path.basename(path)\n",
        "        return img_t, torch.zeros((256,256), dtype=torch.long), name\n",
        "\n",
        "class PseudoCTScanDataset(Dataset):\n",
        "    def __init__(self, image_paths, masks, transform):\n",
        "        assert len(image_paths) == len(masks), \"paths and masks should have the same length\"\n",
        "        self.image_paths = image_paths\n",
        "        self.masks       = masks\n",
        "        self.transform   = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.masks)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        aug = self.transform(image=img, mask=self.masks[i])\n",
        "        return aug['image'], aug['mask']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yadRX84MCDoT"
      },
      "outputs": [],
      "source": [
        "#  Define transformations\n",
        "\n",
        "base_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2(),\n",
        "])\n",
        "\n",
        "\n",
        "weak_augment = A.Compose([\n",
        "    A.RandomCrop(height=192, width=192, p=1.0),\n",
        "    #ou A.RandomRotate90 ou A.Transpose\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=300, sigma=10, p=0.5),\n",
        "        A.GridDistortion(distort_limit=0.2, num_steps=5, p=0.5)],p=1.0)\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), A.ToTensorV2(),])\n",
        "\n",
        "strong_augment = A.Compose([\n",
        "        #A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.4),hole_width_range=(0.1, 0.4), fill_value=0, p=1.0),\n",
        "        A.CoarseDropout(num_holes_range=(4, 8), hole_height_range=(0.1, 0.25),hole_width_range=(0.1, 0.25), fill_value=0, p=1.0),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        A.ToTensorV2(),])\n",
        "\n",
        "\n",
        "#Define Dropout\n",
        "dropout2d = nn.Dropout2d(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEMtOwf0CWYY"
      },
      "outputs": [],
      "source": [
        "#Split train/val\n",
        "\n",
        "full_lab = LabeledCTScanDataset(IMG_DIR, MASK_CSV, base_transform)\n",
        "idxs = np.random.permutation(len(full_lab))\n",
        "split = int(0.8*len(full_lab))\n",
        "train_ds = Subset(full_lab, idxs[:split])\n",
        "val_ds   = Subset(full_lab, idxs[split:])\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "unlab_ds = UnlabeledCTScanDataset(IMG_DIR, MASK_CSV, weak_augment)\n",
        "unlab_loader = DataLoader(unlab_ds, batch_size=BATCH, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "341f8FYXn9_u"
      },
      "outputs": [],
      "source": [
        "model = smp.Segformer(\n",
        "    encoder_name=\"timm-efficientnet-b7\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3, classes=NUM_CLASSES, activation=None\n",
        ").to(DEVICE)\n",
        "\n",
        "loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vRdiQW7GWf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1D7lgO5bLa"
      },
      "source": [
        "## First phase of Unimatch's method :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHq4TaAvn7Bw"
      },
      "outputs": [],
      "source": [
        "best_val = float('inf')\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # train\n",
        "    model.train(); total=0.;total_dice_loss=0.0\n",
        "    for imgs, msks in tqdm(train_loader):\n",
        "        imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        dice_loss = loss_fn(logits, msks.long())\n",
        "        dice_loss.backward()\n",
        "        optimizer.step()\n",
        "        total_dice_loss +=dice_loss.item()*imgs.size(0)\n",
        "\n",
        "\n",
        "    train_dice_loss = total_dice_loss/ len(train_ds)\n",
        "    # val\n",
        "    model.eval(); vtot=0.;vtot_dice_loss=0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, msks in val_loader:\n",
        "            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
        "            vtot_dice_loss+=(loss_fn(model(imgs), msks.long())).item()*imgs.size(0)\n",
        "\n",
        "    val_dice_loss = vtot_dice_loss/len(val_ds)\n",
        "    print(f\"[Epoch {epoch}] Train Loss: {train_dice_loss:.4f}  - Val Loss : {val_dice_loss:4f}\")\n",
        "    if val_dice_loss < best_val:\n",
        "        best_val = val_dice_loss\n",
        "        print(\"→ Meilleur modèle Phase 1 sauvegardé\")\n",
        "        torch.save(model.state_dict(), PATH + \"best_phase_unimatch_threshold06_dropout05_2506.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo2ehPklfy0n"
      },
      "outputs": [],
      "source": [
        "#Load the model\n",
        "model.load_state_dict(torch.load(PATH + \"best_phase_unimatch_threshold06_dropout05_2506.pth\"))\n",
        "model.eval()\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtDJ06O34XrV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMCgIvB1fPox"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#Pseudo-labeling phase\n",
        "\n",
        "def pseudo_labeling(unlab_loader, strong_transform, model):\n",
        "\n",
        "  pseudo_masks = []\n",
        "  pseudo_paths = []\n",
        "\n",
        "  for imgs, _,names in tqdm(unlab_loader, desc=\"Pseudo-labeling\"):  # imgs: (B,3,256,256)\n",
        "      imgs = imgs.to(DEVICE)\n",
        "\n",
        "      # 1) weak view\n",
        "      x_w = imgs\n",
        "\n",
        "      # 2) strong views, image-by-image\n",
        "      x_s1_list, x_s2_list = [], []\n",
        "      for img_tensor in x_w:\n",
        "          # passe de (3,256,256) à (256,256,3) numpy\n",
        "          img_np = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "          s1 = strong_transform(image=img_np)['image']\n",
        "          s2 = strong_transform(image=img_np)['image']\n",
        "          x_s1_list.append(s1)\n",
        "          x_s2_list.append(s2)\n",
        "      x_s1 = torch.stack(x_s1_list).to(DEVICE)  # (B,3,256,256)\n",
        "      x_s2 = torch.stack(x_s2_list).to(DEVICE)  # idem\n",
        "\n",
        "      # 3) encodeur weak → features\n",
        "      feats_w = model.encoder(x_w)         # liste de N maps\n",
        "      last_w  = feats_w[-1]                # map finale (B, C, h, w)\n",
        "      last_fp = dropout2d(last_w)          # feature perturbée\n",
        "      feats_fp = feats_w.copy()\n",
        "      feats_fp[-1] = last_fp\n",
        "\n",
        "      # 4) head segmentation\n",
        "      p_w  = model.segmentation_head(model.decoder(feats_w))   # (B,55,256,256)\n",
        "      p_fp = model.segmentation_head(model.decoder(feats_fp))\n",
        "\n",
        "      # 5) strong views through full model\n",
        "      #    (concat + chunk ou deux appels séparés)\n",
        "      ps   = model(torch.cat([x_s1, x_s2], dim=0))             # (2B,55,256,256)\n",
        "      ps1, ps2 = ps.chunk(2, dim=0)                             # deux views\n",
        "\n",
        "      # 6) calcul du pseudo-mask\n",
        "      conf_w, mask_w = torch.max(torch.softmax(p_w, dim=1), dim=1)  # (B,256,256)\n",
        "      mask_w = mask_w.cpu().numpy()\n",
        "\n",
        "      # 7) seuillage\n",
        "      for c, m in zip(conf_w.cpu(), mask_w):\n",
        "          m[c < 0.6] = 0\n",
        "          pseudo_masks.append(m)\n",
        "\n",
        "      for name in names:\n",
        "            pseudo_paths.append(os.path.join(train_path,name))\n",
        "\n",
        "\n",
        "  pseudo_masks = np.stack(pseudo_masks, axis=0)#concatenate\n",
        "\n",
        "  return pseudo_masks, p_w, p_fp, ps1, ps2\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrWil6wPL094"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "best_val = float('inf')\n",
        "no_improve,patience = 0,10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_masks = []\n",
        "pseudo_paths = []"
      ],
      "metadata": {
        "id": "zxc9HCIjqomJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    sup_loss = 0.0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        l_sup = loss_fn(preds, masks.long())\n",
        "        (l_sup).backward()\n",
        "        optimizer.step()\n",
        "        sup_loss += l_sup.item() * imgs.size(0)\n",
        "    sup_loss /= len(train_loader.dataset)\n",
        "\n",
        "    unsup_loss = 0.0\n",
        "\n",
        "    for imgs, _,names in unlab_loader:  # imgs: (B,3,256,256)\n",
        "      with torch.no_grad():\n",
        "\n",
        "        imgs = imgs.to(DEVICE)\n",
        "\n",
        "        # 1) weak view\n",
        "        x_w = imgs.float()\n",
        "\n",
        "        # 2) strong views, image-by-image\n",
        "        x_s1_list, x_s2_list = [], []\n",
        "        for img_tensor in x_w:\n",
        "            # passe de (3,256,256) à (256,256,3) numpy\n",
        "            img_np = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "            s1 = strong_augment(image=img_np)['image']\n",
        "            s2 = strong_augment(image=img_np)['image']\n",
        "            x_s1_list.append(s1)\n",
        "            x_s2_list.append(s2)\n",
        "        x_s1 = torch.stack(x_s1_list).to(DEVICE)  # (B,3,256,256)\n",
        "        x_s2 = torch.stack(x_s2_list).to(DEVICE)  # idem\n",
        "\n",
        "        # 3) encodeur weak → features\n",
        "        feats_w = model.encoder(x_w)         # liste de N maps\n",
        "        last_w  = feats_w[-1]                # map finale (B, C, h, w)\n",
        "        last_fp = dropout2d(last_w)          # feature perturbée\n",
        "        feats_fp = feats_w.copy()\n",
        "        feats_fp[-1] = last_fp\n",
        "\n",
        "        # 4) head segmentation\n",
        "        p_w  = model.segmentation_head(model.decoder(feats_w))   # (B,55,256,256)\n",
        "        p_fp = model.segmentation_head(model.decoder(feats_fp))\n",
        "\n",
        "        # 5) strong views through full model\n",
        "        #    (concat + chunk ou deux appels séparés)\n",
        "        ps  = model(torch.cat([x_s1, x_s2], dim=0))             # (2B,55,256,256)\n",
        "        ps1, ps2 = ps.chunk(2, dim=0)                             # deux views\n",
        "\n",
        "        # 6) calcul du pseudo-mask\n",
        "        conf_w, masks_w = torch.max(torch.softmax(p_w, dim=1), dim=1)  # (B,256,256)\n",
        "        #mask_w = mask_w.cpu().numpy()\n",
        "\n",
        "        # 7) seuillage\n",
        "        for c, m in zip(conf_w.cpu(), masks_w.cpu().numpy()):\n",
        "            m[c < 0.6] = 0\n",
        "            pseudo_masks.append(m)\n",
        "\n",
        "        for name in names:\n",
        "              pseudo_paths.append(os.path.join(IMG_DIR,name))\n",
        "\n",
        "      #pseudo_masks=np.stack(pseudo_masks,axis=0)\n",
        "\n",
        "\n",
        "      masks_w = masks_w.to(DEVICE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss_u = LAMBDA * loss_fn(p_fp, masks_w.long()) + 0.5 * MU * (loss_fn(ps1, masks_w.long()) + loss_fn(ps2, masks_w.long()))\n",
        "      loss_u.backward()\n",
        "      optimizer.step()\n",
        "      unsup_loss += loss_u.item() * imgs.size(0)\n",
        "    unsup_loss /= len(unlab_loader.dataset)\n",
        "\n",
        "    total_loss =0.5*(sup_loss +  unsup_loss)\n",
        "    print(f\"[Epoch {epoch}] sup={sup_loss:.4f} pseudo={unsup_loss:.4f} tot={total_loss:.4f}\")\n",
        "   #print(f\"[Finetune {epoch}] Train: {train_l:.4f} – Val: {val_l:.4f}\")\n",
        "\n",
        "\n",
        "    if epoch %5 ==0:\n",
        "      torch.save(model.state_dict(), PATH +  f\"best_final_phase_unimatch_threshold06_dropout05_2506.pth_{epoch}.pth\")\n",
        "\n",
        "      print(\"→ Meilleur modèle Final sauvegardé\")\n",
        "\n",
        "print(\"Pipeline semi-supervisé terminée.\")\n",
        "\n",
        "pseudo_masks = np.stack(pseudo_masks, axis=0)\n"
      ],
      "metadata": {
        "id": "V5D3-Qq35m8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjjWdKCupb3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "best_val = float('inf')\n",
        "no_improve,patience = 0,10"
      ],
      "metadata": {
        "id": "luDSYlWGa6l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pseudo_ds)"
      ],
      "metadata": {
        "id": "fGfpxHJlZGik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(full_lab)"
      ],
      "metadata": {
        "id": "YVUwYrAZIoHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mixed_loader)"
      ],
      "metadata": {
        "id": "wzSg_krqIZIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8DbFJie054o"
      },
      "outputs": [],
      "source": [
        "#Final phase of UniMatch\n",
        "\n",
        "for epoch in range(1,NUM_EPOCHS + 1):\n",
        "    model.train(); tot=0.;tot_dice=0.0\n",
        "    for imgs, msks in tqdm(mixed_loader):\n",
        "        imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        #loss = loss_fn(logits, msks.long()) + criterion_sup(logits, msks.long())\n",
        "        dice_loss = loss_fn(logits, msks.long())\n",
        "        #loss.backward(); optimizer.step()\n",
        "        dice_loss.backward(); optimizer.step()\n",
        "        #tot += loss.item()*imgs.size(0)\n",
        "        tot_dice += dice_loss.item()*imgs.size(0)\n",
        "    #scheduler.step(tot/len(mixed_ds))\n",
        "    #train_loss = tot / len(mixed_ds)\n",
        "    train_dice_loss = tot_dice / len(mixed_ds)\n",
        "    # évaluation sur val_loader identique à Phase 1\n",
        "    model.eval(); vtot=0.;vtot_dice=0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, msks in val_loader:\n",
        "            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
        "            #vtot += (loss_fn(model(imgs), msks.long())+criterion_sup(model(imgs), msks.long())).item()*imgs.size(0)\n",
        "            vtot_dice += (loss_fn(model(imgs), msks.long())).item()*imgs.size(0)\n",
        "\n",
        "    #val_loss = vtot / len(val_ds); scheduler.step(val_loss)\n",
        "    val_dice_loss = vtot_dice / len(val_ds); scheduler.step(val_dice_loss)\n",
        "    #print(f\"[Epoch {epoch}] Train: {train_loss:.4f} – Val: {val_loss:.4f}\")\n",
        "    print(f\"[Epoch {epoch}] Train: {train_dice_loss:.4f} – Val: {val_dice_loss:.4f}\")\n",
        "    if val_dice_loss < best_val:\n",
        "        best_val = val_dice_loss\n",
        "        print(\"→ Meilleur modèle phase finale sauvegardé\")\n",
        "        torch.save(model.state_dict(), PATH + \"best_final_unimatch_threshold06_dropout03_1306.pth\")\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve +=1\n",
        "        if no_improve>=patience:\n",
        "            print(f\"--> Early stopping at epoch {epoch}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4qQU8RnzC8i"
      },
      "outputs": [],
      "source": [
        "#0.0782 à epoch 29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Ol19TXocEV"
      },
      "source": [
        "## Prediction phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDQ4DYkCnhc1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgxI5BMYW_0n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4Eg9wqGqmTU"
      },
      "outputs": [],
      "source": [
        "#Save best_simple_segformer_0105.pth\n",
        "#torch.save(model.state_dict(), PATH+ \"best_phase_final_lastepoch_unimatch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sHsyUcxJ1LzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9f7ed5-e49e-4c31-f54c-01682d276b52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#model.load_state_dict(torch.load(PATH +'best_phase2_epoch_1_2706.pth', map_location=DEVICE,weights_only=True))\n",
        "model.load_state_dict(torch.load(PATH +'best_phase2_unimatch_drp05_th0.9_3006.pth', map_location=DEVICE,weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "D_3VbJ7M6f-k"
      },
      "outputs": [],
      "source": [
        "# 5. Inférence sur test set\n",
        "PATH = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# Charger le meilleur modèle\\ n\n",
        "model.eval()\n",
        "\n",
        "# Dataset test (sans masques)\n",
        "class CTTestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")), key=alphanumeric_sort)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.image_paths[idx])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        name = os.path.basename(self.image_paths[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        return img, name\n",
        "\n",
        "# Pas d'augmentation, juste conversion\n",
        "test_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_ds = CTTestDataset(image_dir=os.path.join(PATH, \"test-images\"), transform=test_transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# Inférence et sauvegarde CSV\n",
        "all_preds = []\n",
        "filenames = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0cfesmd14K4u"
      },
      "outputs": [],
      "source": [
        "labels_train = pd.read_csv(\"/content/drive/MyDrive/FewCTSeg/data/y_train.csv\", index_col=0, header=0).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-OCglK2J66oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14675dfd-6f09-41fd-9e46-5e34f8a26fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:04<00:00, 13.72it/s]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(test_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        logits = model(imgs)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()  # (B,H,W)\n",
        "        for p, n in zip(preds, names):\n",
        "            all_preds.append(p.flatten())\n",
        "            filenames.append(n)\n",
        "\n",
        "# Créer DataFrame transposé\n",
        "df = pd.DataFrame(np.stack(all_preds, axis=0), columns=labels_train.columns) #\n",
        "df = df.T\n",
        "# nom des colonnes\n",
        "df.columns = filenames\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Mja-WasR-LrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8870aa6f-7123-4985-d640-1f6c7987cba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions saved to /content/drive/MyDrive/FewCTSeg/data/best_phase2_unimatch_drp05_th0.9_3006.csv\n"
          ]
        }
      ],
      "source": [
        "# Sauvegarder CSV\n",
        "#output_csv = os.path.join(PATH,  'best_phase2_epoch_10_2606.csv')\n",
        "output_csv = os.path.join(PATH,  'best_phase2_unimatch_drp05_th0.9_3006.csv')\n",
        "df.to_csv(output_csv, index=True)\n",
        "print(f\"Test predictions saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(PATH +'best_phase2_final_unimatch_3006.pth', map_location=DEVICE,weights_only=True))"
      ],
      "metadata": {
        "id": "mOLsrladWUi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Inférence sur test set\n",
        "PATH = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# Charger le meilleur modèle\\ n\n",
        "model.eval()\n",
        "\n",
        "# Dataset test (sans masques)\n",
        "class CTTestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")), key=alphanumeric_sort)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.image_paths[idx])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        name = os.path.basename(self.image_paths[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        return img, name\n",
        "\n",
        "# Pas d'augmentation, juste conversion\n",
        "test_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_ds = CTTestDataset(image_dir=os.path.join(PATH, \"test-images\"), transform=test_transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# Inférence et sauvegarde CSV\n",
        "all_preds = []\n",
        "filenames = []\n"
      ],
      "metadata": {
        "id": "LGp7VBEctHJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(test_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        logits = model(imgs)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()  # (B,H,W)\n",
        "        for p, n in zip(preds, names):\n",
        "            all_preds.append(p.flatten())\n",
        "            filenames.append(n)\n",
        "\n",
        "# Créer DataFrame transposé\n",
        "df = pd.DataFrame(np.stack(all_preds, axis=0), columns=labels_train.columns) #\n",
        "df = df.T\n",
        "# nom des colonnes\n",
        "df.columns = filenames\n"
      ],
      "metadata": {
        "id": "j4Sw1NtOWgZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder CSV\n",
        "#output_csv = os.path.join(PATH,  'best_phase2_epoch_10_2606.csv')\n",
        "output_csv = os.path.join(PATH,  'best_phase2_final_unimatch_3006.csv')\n",
        "df.to_csv(output_csv, index=True)\n",
        "print(f\"Test predictions saved to {output_csv}\")"
      ],
      "metadata": {
        "id": "SMs-gnpRWkXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRF Improvement Attempt"
      ],
      "metadata": {
        "id": "nRKa601FtHgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKBSOU1e7Yaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu6b1NGT7Yd-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l6MyOpH7ZDF"
      },
      "source": [
        "## OTHERS VERSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk_H8ljqywYt"
      },
      "outputs": [],
      "source": [
        "#LA PREMIERE VERSION\n",
        "# Chargement Phase1\n",
        "model.load_state_dict(torch.load(PATH + \"best_phase_unimatch.pth\"))\n",
        "model.train()\n",
        "\n",
        "# Génération pseudo-masks\n",
        "pseudo_masks = []\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(unlab_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        # weak / strong augmentations\n",
        "        x_w = imgs\n",
        "        x_s1, x_s2 = x_w.clone(), x_w.clone()\n",
        "        x_s1 = strong_transform(image=x_s1.permute(0,2,3,1).cpu().numpy())['image'].to(DEVICE)\n",
        "        x_s2 = strong_transform(image=x_s2.permute(0,2,3,1).cpu().numpy())['image'].to(DEVICE)\n",
        "        # forward\n",
        "        feat_w = model.encoder(x_w)\n",
        "\n",
        "        #feat_fp = nn.Dropout2d(0.5)(feat_w)\n",
        "        feat_fp = feat_w\n",
        "        p_w, p_fp = model.decoder(torch.cat([feat_w, feat_fp],1)).chunk(2)\n",
        "        ps1, ps2 = model(torch.cat([x_s1, x_s2],0)).chunk(2)\n",
        "        mask = p_w.argmax(1).cpu().numpy()\n",
        "        pseudo_masks.append(mask)\n",
        "pseudo_masks = np.concatenate(pseudo_masks,0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJxHM_Emx7Ft"
      },
      "outputs": [],
      "source": [
        "#LA SECONDE\n",
        "import torch.nn as nn\n",
        "\n",
        "dropout2d = nn.Dropout2d(0.5)\n",
        "\n",
        "pseudo_masks = []\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(unlab_loader, desc=\"Pseudo-labeling\"):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "\n",
        "        # weak + deux vues strong (ici on clone, mais tu peux appliquer des strong transforms)\n",
        "        x_w = imgs\n",
        "        x_s1 = imgs.clone()\n",
        "        x_s2 = imgs.clone()\n",
        "        # … ici tu peux appliquer des strong augmentations à x_s1 / x_s2 …\n",
        "        x_s1 = strong_transform(image=x_s1.permute(0,2,3,1).cpu().numpy())['image'].to(DEVICE)\n",
        "        x_s2 = strong_transform(image=x_s2.permute(0,2,3,1).cpu().numpy())['image'].to(DEVICE)\n",
        "\n",
        "        # 1) Passage encodeur sur la vue weak\n",
        "        feats_w = model.encoder(x_w)        # liste de [f1, f2, ..., fN]\n",
        "        last_w  = feats_w[-1]               # on prend la dernière map\n",
        "        last_fp = dropout2d(last_w)         # feature perturbée\n",
        "        # on remplace la dernière map par la version dropoutée\n",
        "        feats_fp = feats_w.copy()\n",
        "        feats_fp[-1] = last_fp\n",
        "\n",
        "        # 2) Prédictions p_w et p_fp via le décodeur + tête de segmentation\n",
        "        #    (chaque entrée doit être une liste de features)\n",
        "        p_w  = model.segmentation_head( model.decoder(feats_w) )\n",
        "        p_fp = model.segmentation_head( model.decoder(feats_fp) )\n",
        "\n",
        "        # 3) Prédictions sur les deux vues fortes\n",
        "        #    (si ton modèle ne sépare pas facilement, tu peux aussi faire:\n",
        "        #     ps = model(torch.cat([x_s1, x_s2], dim=0)) puis chunk())\n",
        "        ps1 = model(x_s1)\n",
        "        ps2 = model(x_s2)\n",
        "\n",
        "        # 4) Construction du pseudo‐mask (view weak)\n",
        "        conf_w, mask_w = torch.max(torch.softmax(p_w, dim=1), dim=1)\n",
        "        mask_w = mask_w.cpu().numpy()\n",
        "\n",
        "        # 5) Seuillage et collecte\n",
        "        #    (ici on ne garde que sur confiance>0.9, sinon label=0)\n",
        "        for cw, mw in zip(conf_w.cpu(), mask_w):\n",
        "            mw[cw<0.9] = 0\n",
        "            pseudo_masks.append(mw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyvcAaiL5ztT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-A437rgo5roT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Avec corrections du pseudo labeling\n",
        "\"\"\"\n",
        "# on part du principe que :\n",
        "# - Vous avez déjà chargé `filename_to_annotated` depuis annotated_labels.json\n",
        "# - Vous avez un dict `true_masks` qui mappe nom de fichier → masque numpy (uniquement pour les images annotées)\n",
        "# - `unlab_loader` renvoie (img_tensor, _, name)\n",
        "for imgs, _, names in tqdm(unlab_loader, desc=\"Pseudo-labeling\"):\n",
        "    imgs = imgs.to(DEVICE)\n",
        "\n",
        "    # -- weak + perturbed views as before --\n",
        "    x_w = imgs\n",
        "    x_s1_list, x_s2_list = [], []\n",
        "    for img_tensor in x_w:\n",
        "        img_np = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "        x_s1_list.append(strong_transform(image=img_np)['image'])\n",
        "        x_s2_list.append(strong_transform(image=img_np)['image'])\n",
        "    x_s1 = torch.stack(x_s1_list).to(DEVICE)\n",
        "    x_s2 = torch.stack(x_s2_list).to(DEVICE)\n",
        "\n",
        "    feats_w = model.encoder(x_w)\n",
        "    last_w  = feats_w[-1]\n",
        "    feats_fp = feats_w.copy(); feats_fp[-1] = dropout2d(last_w)\n",
        "\n",
        "    p_w = model.segmentation_head(model.decoder(feats_w))  # (B, C, H, W)\n",
        "\n",
        "    # softmax + top-5\n",
        "    probs  = torch.softmax(p_w, dim=1)\n",
        "    top1   = probs.topk(k=1, dim=1)\n",
        "    confs, labels = top1.values.cpu().numpy(), top1.indices.cpu().numpy()\n",
        "    B, _, H, W = labels.shape\n",
        "\n",
        "    for b in range(B):\n",
        "        name = names[b]\n",
        "        # 1) déterminer les labels réellement ABSENTS\n",
        "        #    (parmi ceux annotés pour cette image)\n",
        "        absent_labels = {\n",
        "            L for L in filename_to_annotated[name]\n",
        "            if np.sum(true_masks[name] == L) == 0\n",
        "        }\n",
        "        # 2) on autorise tout autre label\n",
        "        allowed = set(range(NUM_CLASSES)) - absent_labels\n",
        "\n",
        "        # 3) parcours du top-5 pour choisir le 1er candidat autorisé\n",
        "        m_img = np.zeros((H, W), dtype=np.uint8)\n",
        "        for k in range(1):\n",
        "            cand = labels[b, k]    # shape (H,W)\n",
        "            take = (m_img == 0) & np.isin(cand, list(allowed))\n",
        "            m_img[take] = cand[take]\n",
        "            if np.all(m_img != 0):\n",
        "                break\n",
        "\n",
        "        # 4) appliquer votre seuil de confiance sur top-1\n",
        "        low_conf = confs[b,0] < 0.6\n",
        "        m_img[low_conf] = 0\n",
        "\n",
        "        pseudo_masks.append(m_img)\n",
        "        pseudo_paths.append(os.path.join(IMG_DIR, name))\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "sisxuEUr5sAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Better leverage of json\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# on part du principe que :\n",
        "# - Vous avez déjà chargé `filename_to_annotated` depuis annotated_labels.json\n",
        "# - Vous avez un dict `true_masks` qui mappe nom de fichier → masque numpy (uniquement pour les images annotées)\n",
        "# - `unlab_loader` renvoie (img_tensor, _, name)\n",
        "# - `strong_transform`, `dropout2d`, `model`, `DEVICE`, `IMG_DIR`, `NUM_CLASSES` sont définis\n",
        "\n",
        "pseudo_masks = []\n",
        "pseudo_paths = []\n",
        "\n",
        "for imgs, _, names in tqdm(unlab_loader, desc=\"Pseudo-labeling\"):\n",
        "    imgs = imgs.to(DEVICE)\n",
        "\n",
        "    # -- weak + perturbed views as before --\n",
        "    x_w = imgs\n",
        "    x_s1_list, x_s2_list = [], []\n",
        "    for img_tensor in x_w:\n",
        "        img_np = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "        x_s1_list.append(strong_transform(image=img_np)['image'])\n",
        "        x_s2_list.append(strong_transform(image=img_np)['image'])\n",
        "    x_s1 = torch.stack(x_s1_list).to(DEVICE)\n",
        "    x_s2 = torch.stack(x_s2_list).to(DEVICE)\n",
        "\n",
        "    feats_w = model.encoder(x_w)\n",
        "    last_w  = feats_w[-1]\n",
        "    feats_fp = feats_w.copy(); feats_fp[-1] = dropout2d(last_w)\n",
        "\n",
        "    p_w = model.segmentation_head(model.decoder(feats_w))  # (B, C, H, W)\n",
        "\n",
        "    # softmax + top-5\n",
        "    probs  = torch.softmax(p_w, dim=1)\n",
        "    top5   = probs.topk(k=5, dim=1)\n",
        "    confs, labels = top5.values.cpu().numpy(), top5.indices.cpu().numpy()\n",
        "    B, _, H, W = labels.shape\n",
        "\n",
        "    for b in range(B):\n",
        "        name = names[b]\n",
        "        # 1) déterminer les labels réellement ABSENTS\n",
        "        #    (parmi ceux annotés pour cette image)\n",
        "        absent_labels = {\n",
        "            L for L in filename_to_annotated[name]\n",
        "            if np.sum(true_masks[name] == L) == 0\n",
        "        }\n",
        "        # 2) on autorise tout autre label\n",
        "        allowed = set(range(NUM_CLASSES)) - absent_labels\n",
        "\n",
        "        # 3) parcours du top-5 pour choisir le 1er candidat autorisé\n",
        "        m_img = np.zeros((H, W), dtype=np.uint8)\n",
        "        for k in range(5):\n",
        "            cand = labels[b, k]    # shape (H,W)\n",
        "            take = (m_img == 0) & np.isin(cand, list(allowed))\n",
        "            m_img[take] = cand[take]\n",
        "            if np.all(m_img != 0):\n",
        "                break\n",
        "\n",
        "        # 4) appliquer votre seuil de confiance sur top-1\n",
        "        low_conf = confs[b,0] < 0.6\n",
        "        m_img[low_conf] = 0\n",
        "\n",
        "        pseudo_masks.append(m_img)\n",
        "        pseudo_paths.append(os.path.join(IMG_DIR, name))\n",
        "\n",
        "# Empiler et créer le dataset\n",
        "pseudo_masks = np.stack(pseudo_masks, axis=0)\n",
        "class PseudoCTScanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_paths, masks, transform):\n",
        "        assert len(image_paths)==len(masks)\n",
        "        self.image_paths = image_paths\n",
        "        self.masks       = masks\n",
        "        self.transform   = transform\n",
        "    def __len__(self):\n",
        "        return len(self.masks)\n",
        "    def __getitem__(self, i):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        aug = self.transform(image=img, mask=self.masks[i])\n",
        "        return aug['image'], aug['mask']\n",
        "\n",
        "pseudo_ds    = PseudoCTScanDataset(pseudo_paths, pseudo_masks, base_transform)\n",
        "mixed_ds     = ConcatDataset([full_lab, pseudo_ds])\n",
        "mixed_loader = DataLoader(mixed_ds, batch_size=BATCH, shuffle=True, num_workers=4)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OuaHWtcI8Ee7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRF\n",
        "\"\"\"\n",
        "!pip install -U segmentation-models-pytorch pydensecrf -q\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_gaussian, create_pairwise_bilateral\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "\n",
        "# ---------------------------\n",
        "#  Réglages et chemins\n",
        "# ---------------------------\n",
        "SEED        = 26\n",
        "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PATH_DATA   = \"/content/drive/MyDrive/FewCTSeg/data/\"\n",
        "IMG_DIR     = os.path.join(PATH_DATA, \"train-images\")\n",
        "TEST_DIR    = os.path.join(PATH_DATA, \"test-images\")\n",
        "MASK_CSV    = os.path.join(PATH_DATA, \"y_train.csv\")\n",
        "MODEL_PATH  = os.path.join(PATH_DATA, \"best_final_unimatch_threshold06_dropout03_1306.pth\")\n",
        "BATCH       = 8\n",
        "NUM_CLASSES = 55\n",
        "\n",
        "# ---------------------------\n",
        "#  Helpers\n",
        "# ---------------------------\n",
        "def alphanumeric_sort(name):\n",
        "    parts = re.split(r\"(\\d+)\", name)\n",
        "    return [int(p) if p.isdigit() else p for p in parts]\n",
        "\n",
        "def apply_crf(image_np, prob_map_np,\n",
        "              gaussian_sxy=(3,3), bilateral_sxy=(80,80), bilateral_srgb=(13,13,13),\n",
        "              num_iters=5):\n",
        "    \"\"\"\n",
        "    image_np : HxWx3 uint8\n",
        "    prob_map_np : CxHxW float32 (softmax probabilities)\n",
        "    \"\"\"\n",
        "    C, H, W = prob_map_np.shape\n",
        "    d = dcrf.DenseCRF2D(W, H, C)\n",
        "    unary = unary_from_softmax(prob_map_np)\n",
        "    d.setUnaryEnergy(unary)\n",
        "    # spatial term\n",
        "    feats = create_pairwise_gaussian(sdims=gaussian_sxy, shape=(H,W))\n",
        "    d.addPairwiseEnergy(feats, compat=3)\n",
        "    # bilateral term\n",
        "    feats = create_pairwise_bilateral(sdims=bilateral_sxy,\n",
        "                                      schan=bilateral_srgb,\n",
        "                                      img=image_np,\n",
        "                                      chdim=2)\n",
        "    d.addPairwiseEnergy(feats, compat=10)\n",
        "    Q = d.inference(num_iters)\n",
        "    return np.array(Q).reshape((C, H, W))\n",
        "\n",
        "# ---------------------------\n",
        "#  Dataset test\n",
        "# ---------------------------\n",
        "class CTTestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir   = image_dir\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")),\n",
        "                                   key=alphanumeric_sort)\n",
        "        self.transform   = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        img  = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        name = os.path.basename(path)\n",
        "        img_t = self.transform(image=img)[\"image\"] if self.transform else img\n",
        "        return img_t, name\n",
        "\n",
        "# ---------------------------\n",
        "#  Pré‑traitements\n",
        "# ---------------------------\n",
        "test_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485,0.456,0.406),\n",
        "                std=(0.229,0.224,0.225)),\n",
        "    A.ToTensorV2(),\n",
        "], seed=SEED)\n",
        "\n",
        "# ---------------------------\n",
        "#  Chargement du modèle\n",
        "# ---------------------------\n",
        "model = smp.Segformer(\n",
        "    encoder_name=\"timm-efficientnet-b7\",\n",
        "    encoder_weights=None,\n",
        "    in_channels=3, classes=NUM_CLASSES, activation=None\n",
        ").to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "# ---------------------------\n",
        "#  DataLoader test\n",
        "# ---------------------------\n",
        "test_ds     = CTTestDataset(TEST_DIR, transform=test_transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "# ---------------------------\n",
        "#  Inférence + CRF + CSV\n",
        "# ---------------------------\n",
        "all_preds = []\n",
        "filenames = []\n",
        "\n",
        "# On récupère les colonnes de sortie (la même disposition que y_train.csv)\n",
        "labels_train = pd.read_csv(MASK_CSV, index_col=0).T\n",
        "columns = labels_train.columns\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(test_loader, desc=\"Test w/ CRF\"):\n",
        "        imgs = imgs.to(DEVICE)                       # (B,3,H,W)\n",
        "        logits = model(imgs)                         # (B,C,H,W)\n",
        "        probs  = F.softmax(logits, dim=1).cpu().numpy()  # (B,C,H,W)\n",
        "\n",
        "        B, C, H, W = probs.shape\n",
        "        for b in range(B):\n",
        "            # 1) lire l'image originale (uint8) pour le CRF\n",
        "            orig_path  = os.path.join(TEST_DIR, names[b])\n",
        "            img_np     = cv2.cvtColor(cv2.imread(orig_path), cv2.COLOR_BGR2RGB)\n",
        "            # 2) appliquer CRF\n",
        "            crf_refined = apply_crf(img_np, probs[b])\n",
        "            mask_crf    = np.argmax(crf_refined, axis=0)  # (H,W)\n",
        "            # 3) aplatir et stocker\n",
        "            all_preds.append(mask_crf.flatten())\n",
        "            filenames.append(names[b])\n",
        "\n",
        "# Assemblage du DataFrame et sauvegarde\n",
        "df = pd.DataFrame(np.stack(all_preds, axis=0), columns=columns)\n",
        "df = df.T\n",
        "df.columns = filenames\n",
        "\n",
        "output_csv = os.path.join(PATH_DATA, \"best_final_unimatch_threshold06_crf.csv\")\n",
        "df.to_csv(output_csv, index=True)\n",
        "print(f\"Test predictions with CRF saved to {output_csv}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4GpxxOwF8IqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2079a0f9a6774930a4cd354ad62a1851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c187bd4f62044a09ab03342b6fbbaf1a",
              "IPY_MODEL_5a69e5f66442413287d917043e5c443a",
              "IPY_MODEL_297687afb346474eb83ba6d27ffa6f61"
            ],
            "layout": "IPY_MODEL_b2df93a9c8ab4ab69da785a851166bf4"
          }
        },
        "c187bd4f62044a09ab03342b6fbbaf1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d178fe1ea36545b98820d502daf2a2e2",
            "placeholder": "​",
            "style": "IPY_MODEL_e18c366f2d334fc8b184f5172939647a",
            "value": "config.json: 100%"
          }
        },
        "5a69e5f66442413287d917043e5c443a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9838ad30cc564bbeb2274d37e3a358b8",
            "max": 106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a305423b90a4623ac0fb045a9db61bb",
            "value": 106
          }
        },
        "297687afb346474eb83ba6d27ffa6f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b827ab012e8d42718b66a90f8daca307",
            "placeholder": "​",
            "style": "IPY_MODEL_32016ab5b3f049f6b8baa322982a8a98",
            "value": " 106/106 [00:00&lt;00:00, 12.8kB/s]"
          }
        },
        "b2df93a9c8ab4ab69da785a851166bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d178fe1ea36545b98820d502daf2a2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18c366f2d334fc8b184f5172939647a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9838ad30cc564bbeb2274d37e3a358b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a305423b90a4623ac0fb045a9db61bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b827ab012e8d42718b66a90f8daca307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32016ab5b3f049f6b8baa322982a8a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d49c919d3aa0407888ffb8c7ecc5aa8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5f4bba395f545c1ada03c1f7d9bf1f6",
              "IPY_MODEL_f9f1e26539a0445dad72f96e8c7b3cea",
              "IPY_MODEL_6c5bd7237d474c7cb75e0d0ae9776032"
            ],
            "layout": "IPY_MODEL_f4a66aaa174a47b4bea5db16db0f9623"
          }
        },
        "d5f4bba395f545c1ada03c1f7d9bf1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d4de8a58043465399ec54a007f74195",
            "placeholder": "​",
            "style": "IPY_MODEL_13f0a56fc5fa488ea0bf26e0421a8e92",
            "value": "model.safetensors: 100%"
          }
        },
        "f9f1e26539a0445dad72f96e8c7b3cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd31b9d2f73e40a3af6c38e74d57fcc9",
            "max": 266748384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24ab1aa6916c4f3b963048ffd0ea6b53",
            "value": 266748384
          }
        },
        "6c5bd7237d474c7cb75e0d0ae9776032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_070c5efd3a22416593abddd73c5dbcca",
            "placeholder": "​",
            "style": "IPY_MODEL_1f54a26692124a6f8eca390e35abb26f",
            "value": " 267M/267M [00:01&lt;00:00, 221MB/s]"
          }
        },
        "f4a66aaa174a47b4bea5db16db0f9623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4de8a58043465399ec54a007f74195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f0a56fc5fa488ea0bf26e0421a8e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd31b9d2f73e40a3af6c38e74d57fcc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ab1aa6916c4f3b963048ffd0ea6b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "070c5efd3a22416593abddd73c5dbcca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f54a26692124a6f8eca390e35abb26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e6ff2b3bf234508b355dfd7ef35a2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_844a881786ba481786ea053e3841020c",
              "IPY_MODEL_31cf7cf88d944391be747129652c924e",
              "IPY_MODEL_89e30304a9714bd7bc7ef232ec3aa42a"
            ],
            "layout": "IPY_MODEL_4c48e89397574068bc9c546eb43682dd"
          }
        },
        "844a881786ba481786ea053e3841020c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_617c79783719474db5fdc3b0558d321f",
            "placeholder": "​",
            "style": "IPY_MODEL_869ba79974094bd18fe207b284140df1",
            "value": "config.json: 100%"
          }
        },
        "31cf7cf88d944391be747129652c924e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ed70e6ea4f4dac8b2f0bdb0b74aaa9",
            "max": 106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78c12c8de646424982cb3edfd0288261",
            "value": 106
          }
        },
        "89e30304a9714bd7bc7ef232ec3aa42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b13e72359f94d298d0e9147c9843abc",
            "placeholder": "​",
            "style": "IPY_MODEL_247cefd1cca4447cac05f4d661981bfa",
            "value": " 106/106 [00:00&lt;00:00, 10.9kB/s]"
          }
        },
        "4c48e89397574068bc9c546eb43682dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "617c79783719474db5fdc3b0558d321f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869ba79974094bd18fe207b284140df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3ed70e6ea4f4dac8b2f0bdb0b74aaa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c12c8de646424982cb3edfd0288261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b13e72359f94d298d0e9147c9843abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "247cefd1cca4447cac05f4d661981bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0df51014a3cb4409b5d31de150eedd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83803dcec02e4fb8ba0599289ab9cd16",
              "IPY_MODEL_34532f89291745bb972f302e4be3b392",
              "IPY_MODEL_502b95d695ee49379703df1130e625dc"
            ],
            "layout": "IPY_MODEL_44012c35eb45403ab35e0f4b3e164609"
          }
        },
        "83803dcec02e4fb8ba0599289ab9cd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_172800da2eeb40b08f1d5b05ce31a2e6",
            "placeholder": "​",
            "style": "IPY_MODEL_4712bb2141464f7aa3aff0baf3c2b139",
            "value": "model.safetensors: 100%"
          }
        },
        "34532f89291745bb972f302e4be3b392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fcf0b0a9b5457ab7f5a9f71f5d5d65",
            "max": 266748384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f04242f63865415f84d14dd85ce725d3",
            "value": 266748384
          }
        },
        "502b95d695ee49379703df1130e625dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472c10561cb048859821ffc7fffec819",
            "placeholder": "​",
            "style": "IPY_MODEL_a79e14d5391c4001b5ba9c5bd484ce65",
            "value": " 267M/267M [00:00&lt;00:00, 288MB/s]"
          }
        },
        "44012c35eb45403ab35e0f4b3e164609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172800da2eeb40b08f1d5b05ce31a2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4712bb2141464f7aa3aff0baf3c2b139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10fcf0b0a9b5457ab7f5a9f71f5d5d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04242f63865415f84d14dd85ce725d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "472c10561cb048859821ffc7fffec819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79e14d5391c4001b5ba9c5bd484ce65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}