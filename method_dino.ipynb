{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0975ff86-2882-44b3-89a8-5cc3a46dd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from utils_functions.sort_files import alphanumeric_sort\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Réglage aléatoire\n",
    "# ---------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6e5c41-8bff-4f2e-899f-e54596464673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Chemins des données\n",
    "# ---------------------------\n",
    "#PATH = \"./data/\"\n",
    "#train_images = sorted(glob.glob(os.path.join(PATH, \"train-images\", \"*.png\")))\n",
    "\n",
    "# MONAI imports corrigés\n",
    "# MONAI imports corrigés\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    Resized,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceCELoss\n",
    "#from monai.metrics import compute_meandice\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Réglage aléatoire\n",
    "# ---------------------------\n",
    "set_determinism(seed=42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------------------------\n",
    "# Chemins des données\n",
    "# ---------------------------\n",
    "PATH = \"./data/\"\n",
    "train_images = sorted(glob.glob(os.path.join(PATH, \"train-images\", \"*.png\")),key=alphanumeric_sort)\n",
    "mask_csv = os.path.join(PATH, \"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8489e653-1cc1-4bd0-8942-4c43e2fc6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from monai.metrics.utils import do_metric_reduction, ignore_background\n",
    "from monai.utils import MetricReduction\n",
    "\n",
    "\n",
    "class DiceMetric:\n",
    "    \"\"\"\n",
    "    Compute average Dice loss between two tensors. It can support both multi-classes and multi-labels tasks.\n",
    "    Input `y_pred` (BNHW[D] where N is number of classes) is compared with ground truth `y` (BNHW[D]).\n",
    "    `y_preds` is expected to have binarized predictions and `y` should be in one-hot format. You can use suitable transforms\n",
    "    in ``monai.transforms.post`` first to achieve binarized values.\n",
    "    The `include_background` parameter can be set to ``False`` for an instance of DiceLoss to exclude\n",
    "    the first category (channel index 0) which is by convention assumed to be background. If the non-background\n",
    "    segmentations are small compared to the total image size they can get overwhelmed by the signal from the\n",
    "    background so excluding it in such cases helps convergence.\n",
    "\n",
    "    Args:\n",
    "        include_background: whether to skip Dice computation on the first channel of\n",
    "            the predicted output. Defaults to ``True``.\n",
    "        reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``, ``\"mean_batch\"``, ``\"sum_batch\"``,\n",
    "            ``\"mean_channel\"``, ``\"sum_channel\"``}\n",
    "            Define the mode to reduce computation result of 1 batch data. Defaults to ``\"mean\"``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_background: bool = True,\n",
    "        reduction: Union[MetricReduction, str] = MetricReduction.MEAN,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.include_background = include_background\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, y_pred: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: input data to compute, typical segmentation model output.\n",
    "                It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values\n",
    "                should be binarized.\n",
    "            y: ground truth to compute mean dice metric. It must be one-hot format and first dim is batch.\n",
    "                The values should be binarized.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: when `y` is not a binarized tensor.\n",
    "            ValueError: when `y_pred` has less than three dimensions.\n",
    "        \"\"\"\n",
    "        if not torch.all(y_pred.byte() == y_pred):\n",
    "            warnings.warn(\"y_pred is not a binarized tensor here!\")\n",
    "        if not torch.all(y.byte() == y):\n",
    "            raise ValueError(\"y should be a binarized tensor.\")\n",
    "        dims = y_pred.ndimension()\n",
    "        if dims < 3:\n",
    "            raise ValueError(\"y_pred should have at least three dimensions.\")\n",
    "        # compute dice (BxC) for each channel for each batch\n",
    "        f = compute_meandice(\n",
    "            y_pred=y_pred,\n",
    "            y=y,\n",
    "            include_background=self.include_background,\n",
    "        )\n",
    "\n",
    "        # do metric reduction\n",
    "        f, not_nans = do_metric_reduction(f, self.reduction)\n",
    "        return f, not_nans\n",
    "\n",
    "\n",
    "\n",
    "def compute_meandice(\n",
    "    y_pred: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    include_background: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes Dice score metric from full size Tensor and collects average.\n",
    "\n",
    "    Args:\n",
    "        y_pred: input data to compute, typical segmentation model output.\n",
    "            It must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32]. The values\n",
    "            should be binarized.\n",
    "        y: ground truth to compute mean dice metric. It must be one-hot format and first dim is batch.\n",
    "            The values should be binarized.\n",
    "        include_background: whether to skip Dice computation on the first channel of\n",
    "            the predicted output. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Dice scores per batch and per class, (shape [batch_size, n_classes]).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when `y_pred` and `y` have different shapes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not include_background:\n",
    "        y_pred, y = ignore_background(\n",
    "            y_pred=y_pred,\n",
    "            y=y,\n",
    "        )\n",
    "\n",
    "    y = y.float()\n",
    "    y_pred = y_pred.float()\n",
    "\n",
    "    if y.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_pred and y should have same shapes.\")\n",
    "\n",
    "    # reducing only spatial dimensions (not batch nor channels)\n",
    "    n_len = len(y_pred.shape)\n",
    "    reduce_axis = list(range(2, n_len))\n",
    "    intersection = torch.sum(y * y_pred, dim=reduce_axis)\n",
    "\n",
    "    y_o = torch.sum(y, reduce_axis)\n",
    "    y_pred_o = torch.sum(y_pred, dim=reduce_axis)\n",
    "    denominator = y_o + y_pred_o\n",
    "\n",
    "    f = torch.where(y_o > 0, (2.0 * intersection) / denominator, torch.tensor(float(\"nan\"), device=y_o.device))\n",
    "    return f  # returns array of Dice with shape: [batch, n_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f81ffb-ffc5-4bb1-84c0-09b66521cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger tous les masques en mémoire\n",
    "masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1, 256, 256).astype(np.int64)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Dataset MONAI\n",
    "# ---------------------------\n",
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, image_paths, masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        mask = self.masks[idx].astype(np.int64)\n",
    "\n",
    "        sample = {\"image\": img, \"mask\": mask}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# Transforms MONAI dict-based\n",
    "train_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    ScaleIntensityd(keys=[\"image\"]),               # normaliser l'intensité de l'image\n",
    "    Resized(keys=[\"image\", \"mask\"], spatial_size=[256, 256]),\n",
    "    ToTensord(keys=[\"image\", \"mask\"]),           # convertir en tenseurs\n",
    "])\n",
    "\n",
    "# Créer dataset et dataloader\n",
    "train_ds = CTScanDataset(train_images, masks, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a56845-ec78-4e5b-b599-217c85adf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 2. Modèle MONAI UNet\n",
    "# ---------------------------\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=55,\n",
    "    channels=(32, 64, 128, 256, 512),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Perte et métrique\n",
    "# ---------------------------\n",
    "loss_fn = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "metric_fn = compute_meandice\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Entraînement\n",
    "# ---------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed8172c-c005-4417-baa6-34ce2cf0bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bb7dd-3fa2-4c70-86d1-cf91631f3b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/250 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        imgs = batch['image'].to(device)          # (B,1,256,256)\n",
    "        labels = batch['mask'].to(device)         # (B,1,256,256)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)                     # (B,55,256,256)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # calcul de Mean Dice sur un mini-batch pour suivi rapide\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = next(iter(train_loader))\n",
    "        imgs = sample['image'].to(device)\n",
    "        labels = sample['mask'].to(device)\n",
    "        preds = model(imgs)\n",
    "        mean_dice = metric_fn(\n",
    "            y_pred=preds, y=labels,\n",
    "            include_background=True,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True\n",
    "        ).mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Loss: {avg_loss:.4f} — Mean Dice: {mean_dice:.4f}\")\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "torch.save(model.state_dict(), \"unet_monai.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45a026-2adb-4f31-b0f0-55b013288094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412caca7-c2de-431d-9254-1b3a8353dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Chemins des données\n",
    "# ---------------------------\n",
    "PATH = \"./data/\"\n",
    "train_path = os.path.join(PATH, \"train-images/\")\n",
    "mask_csv = os.path.join(PATH, \"y_train.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Dataset Definition\n",
    "# ---------------------------\n",
    "class HFCTScanDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_csv, image_processor):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")),key=alphanumeric_sort)\n",
    "        # Charger masques transposés\n",
    "        masks = pd.read_csv(mask_csv, index_col=0).T.values\n",
    "        self.masks = masks.reshape(-1, 256, 256).astype(np.int64)\n",
    "        self.processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ouvrir image grayscale -> convertir en 3 canaux\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = np.stack([np.array(img)]*3, axis=-1)\n",
    "        mask = self.masks[idx]\n",
    "        # préparer pixel_values\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs.pixel_values.squeeze(0)  # (3, H, W)\n",
    "        # HuggingFace attend (H, W) labels\n",
    "        label = torch.from_numpy(mask)\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": label}\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Feature Extractor & Model\n",
    "# ---------------------------\n",
    "checkpoint = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint,from_tf=True)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=55,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    from_tf=True\n",
    "    \n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Metrics (pixel accuracy)\n",
    "# ---------------------------\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    # calcul d'accuracy pixel-wise\n",
    "    valid = labels >= 0\n",
    "    acc = (preds[valid] == labels[valid]).sum() / valid.sum()\n",
    "    return {\"pixel_accuracy\": acc}\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Entraînement\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6516ae-cf3d-4733-9718-3b222edb7793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to C:\\Users\\rmondelice/.cache\\torch\\hub\\checkpoints\\dinov2_vits14_pretrain.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 84.2M/84.2M [05:55<00:00, 249kB/s]\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb88fb9-0a4e-4c3e-8408-78a74b08e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_functions.sort_files import alphanumeric_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b8e64-59ee-4476-b767-cfa0cd986ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\rmondelice/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Charger DINOv2 depuis torch.hub\n",
    "dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', pretrained=True)\n",
    "dinov2_model.eval()\n",
    "\n",
    "# ---------------------------\n",
    "# Chemins des données\n",
    "# ---------------------------\n",
    "PATH = \"./data/\"\n",
    "train_images = sorted(glob.glob(os.path.join(PATH, \"train-images\", \"*.png\")),key=alphanumeric_sort)\n",
    "mask_csv = os.path.join(PATH, \"y_train.csv\")\n",
    "\n",
    "# Charger tous les masques en mémoire\n",
    "masks = pd.read_csv(mask_csv, index_col=0).T.values.reshape(-1, 256, 256).astype(np.int64)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Dataset\n",
    "# ---------------------------\n",
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, image_paths, masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # dupliquer en 3 canaux pour DINO\n",
    "        img3 = np.stack([img]*3, axis=0)\n",
    "        mask = self.masks[idx]\n",
    "        return torch.from_numpy(img3), torch.from_numpy(mask)\n",
    "\n",
    "# Transforms: resize et normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # convertit en [C,H,W]\n",
    "    transforms.Resize((224,224)),   # DINOv2 attend 224x224\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "train_ds = CTScanDataset(train_images, masks, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Segmentation model\n",
    "# ---------------------------\n",
    "class Dinov2Seg(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=55):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # Freeze backbone\n",
    "        for p in self.backbone.parameters(): p.requires_grad = False\n",
    "        # conv to reduce embed dim to num_classes\n",
    "        embed_dim = self.backbone.embed_dim  # 384 pour vits14\n",
    "        self.conv1 = nn.Conv2d(embed_dim, 256, kernel_size=1)\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,224,224)\n",
    "        # obtenir patches embeddings\n",
    "        feats = self.backbone.forward_features(x)  # (B, P+1, D)\n",
    "        feats = feats[:,1:,:]                      # retirer token CLS\n",
    "        B, P, D = feats.shape\n",
    "        h = w = int(np.sqrt(P))\n",
    "        feats = feats.permute(0,2,1).contiguous().view(B, D, h, w)  # (B,D,h,w)\n",
    "        # upsample aux dimensions d'origine 256\n",
    "        y = self.conv1(feats)\n",
    "        y = self.up1(y)   # x2\n",
    "        y = self.up2(y)   # x4\n",
    "        y = self.up3(y)   # x8\n",
    "        # feats 224->28 patches, h=w=14 -> after up1:28, up2:56, up3:112\n",
    "        # on resize final à 256\n",
    "        y = self.out_conv(y)\n",
    "        y = nn.functional.interpolate(y, size=(256,256), mode='bilinear', align_corners=False)\n",
    "        return y  # (B, num_classes, 256,256)\n",
    "\n",
    "# Instanciation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Dinov2Seg(dinov2_model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d199c83-cdcf-4701-9cd7-6c788feca304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/250 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# ---------------------------\n",
    "# 3. Training\n",
    "# ---------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, masks in tqdm(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "torch.save(model.state_dict(), \"dinov2_seg.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7e6c3-a3fd-4838-af7b-1ea686ce49e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
